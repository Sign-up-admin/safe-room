# è‡ªåŠ¨åŒ–è¿ç»´æŒ‡å—

> **ç‰ˆæœ¬**: v1.0
> **æœ€åæ›´æ–°**: 2025-11-16
> **ç»´æŠ¤è€…**: è¿ç»´å›¢é˜Ÿ

## æ¦‚è¿°

æœ¬æŒ‡å—æ¶µç›–å¥èº«æˆ¿ç®¡ç†ç³»ç»Ÿçš„è‡ªåŠ¨åŒ–è¿ç»´ä½“ç³»ï¼ŒåŒ…æ‹¬CI/CDæµæ°´çº¿ã€ç›‘æ§å‘Šè­¦ã€å¤‡ä»½æ¢å¤ã€éƒ¨ç½²è‡ªåŠ¨åŒ–ç­‰æ ¸å¿ƒç»„ä»¶ï¼Œä¸ºç³»ç»Ÿæä¾›7Ã—24å°æ—¶ç¨³å®šè¿è¡Œä¿éšœã€‚

## ç›®å½•

- [1. CI/CD è‡ªåŠ¨åŒ–](#1-cicd-è‡ªåŠ¨åŒ–)
- [2. éƒ¨ç½²è‡ªåŠ¨åŒ–](#2-éƒ¨ç½²è‡ªåŠ¨åŒ–)
- [3. ç›‘æ§å’Œå‘Šè­¦è‡ªåŠ¨åŒ–](#3-ç›‘æ§å’Œå‘Šè­¦è‡ªåŠ¨åŒ–)
- [4. å¤‡ä»½æ¢å¤è‡ªåŠ¨åŒ–](#4-å¤‡ä»½æ¢å¤è‡ªåŠ¨åŒ–)
- [5. åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–](#5-åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–)
- [6. å®‰å…¨è‡ªåŠ¨åŒ–](#6-å®‰å…¨è‡ªåŠ¨åŒ–)
- [7. æ–‡æ¡£è‡ªåŠ¨åŒ–](#7-æ–‡æ¡£è‡ªåŠ¨åŒ–)
- [8. è¿ç»´æµç¨‹è‡ªåŠ¨åŒ–](#8-è¿ç»´æµç¨‹è‡ªåŠ¨åŒ–)

---

## 1. CI/CD è‡ªåŠ¨åŒ–

### 1.1 GitHub Actions å·¥ä½œæµ

#### å®Œæ•´ CI/CD æµæ°´çº¿

```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  # ä»£ç è´¨é‡æ£€æŸ¥
  quality:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
      - name: Install dependencies
        run: npm ci
      - name: Lint code
        run: npm run lint
      - name: Run tests
        run: npm test
      - name: Security scan
        run: npm audit --audit-level high

  # æ„å»ºå’Œå®¹å™¨åŒ–
  build:
    needs: quality
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Build Docker images
        run: |
          docker build -t fitness-gym-api:${{ github.sha }} ./api
          docker build -t fitness-gym-frontend:${{ github.sha }} ./frontend
      - name: Run integration tests
        run: docker-compose -f docker-compose.test.yml up --abort-on-container-exit
      - name: Push to registry
        run: |
          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
          docker push fitness-gym-api:${{ github.sha }}
          docker push fitness-gym-frontend:${{ github.sha }}

  # éƒ¨ç½²åˆ°ä¸åŒç¯å¢ƒ
  deploy-staging:
    needs: build
    if: github.ref == 'refs/heads/develop'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Deploy to staging
        run: |
          ssh user@staging-server << EOF
            cd /opt/fitness-gym
            docker-compose pull
            docker-compose up -d
            ./docker-deploy.sh health
          EOF

  deploy-production:
    needs: build
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Deploy to production
        run: |
          ssh user@prod-server << EOF
            cd /opt/fitness-gym
            ./docker-deploy.sh up --environment production --blue-green
          EOF
```

#### æ–‡æ¡£å‘å¸ƒå·¥ä½œæµ

```yaml
# .github/workflows/docs-publish.yml
name: Documentation

on:
  push:
    branches: [ main ]
    paths:
      - 'docs/**'

jobs:
  publish-docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
      - name: Validate documentation
        run: npm run validate-docs
      - name: Generate docs
        run: npm run generate-docs
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs/build
```

### 1.2 è‡ªåŠ¨åŒ–æµ‹è¯•ç­–ç•¥

#### æµ‹è¯•é‡‘å­—å¡”ç»“æ„

```
ç«¯åˆ°ç«¯æµ‹è¯• (E2E)     â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  (20%)
é›†æˆæµ‹è¯• (Integration) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (35%)
å•å…ƒæµ‹è¯• (Unit)       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (45%)
```

#### è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬

```bash
#!/bin/bash
# scripts/run-tests.sh

echo "ğŸƒ è¿è¡Œæµ‹è¯•å¥—ä»¶..."

# å•å…ƒæµ‹è¯•
npm test -- --coverage --watchAll=false

# é›†æˆæµ‹è¯•
docker-compose -f docker-compose.test.yml up --abort-on-container-exit

# E2E æµ‹è¯•
npx cypress run --record --parallel

# æ€§èƒ½æµ‹è¯•
npx artillery run performance-test.yml

echo "âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ"
```

### 1.3 ä»£ç è´¨é‡é—¨ç¦

#### è´¨é‡æ£€æŸ¥é…ç½®

```yaml
# .github/workflows/quality-gate.yml
name: Quality Gate

on: [push, pull_request]

jobs:
  quality-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Code quality checks
        uses: github/super-linter@v5
        env:
          DEFAULT_BRANCH: main
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Test coverage
        run: |
          npm run test:coverage
          # è¦æ±‚æœ€ä½è¦†ç›–ç‡ 80%
          npx istanbul check-coverage --statements 80 --branches 75 --functions 80 --lines 80
```

---

## 2. éƒ¨ç½²è‡ªåŠ¨åŒ–

### 2.1 å¢å¼ºéƒ¨ç½²è„šæœ¬

#### Bash éƒ¨ç½²è„šæœ¬åŠŸèƒ½

```bash
#!/bin/bash
# docker-deploy.sh

SCRIPT_VERSION="2.0.0"
LOG_FILE="/var/log/fitness-gym-deploy.log"
DEPLOYMENT_BACKUP_DIR="/opt/fitness-gym/backups"

# ç¯å¢ƒéªŒè¯
validate_environment() {
    echo "ğŸ” éªŒè¯ç¯å¢ƒ..."
    check_system_resources
    check_port_availability
    validate_environment_variables
}

# é¢„éƒ¨ç½²æ£€æŸ¥
pre_deployment_checks() {
    echo "ğŸ›¡ï¸  æ‰§è¡Œé¢„éƒ¨ç½²æ£€æŸ¥..."
    create_deployment_backup
    backup_database
    stop_services_gracefully
}

# éƒ¨ç½²æ‰§è¡Œ
perform_deployment() {
    echo "ğŸš€ æ‰§è¡Œéƒ¨ç½²..."
    pull_images
    update_configuration
    start_services
    run_database_migrations
}

# å¥åº·æ£€æŸ¥
perform_health_checks() {
    echo "ğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥..."
    wait_for_services
    run_health_checks
    validate_deployment
}

# å›æ»šåŠŸèƒ½
perform_rollback() {
    echo "âª æ‰§è¡Œå›æ»š..."
    stop_current_services
    restore_from_backup
    start_previous_version
    run_health_checks
}

# ç”Ÿæˆéƒ¨ç½²æŠ¥å‘Š
generate_deployment_report() {
    echo "ğŸ“Š ç”Ÿæˆéƒ¨ç½²æŠ¥å‘Š..."
    collect_deployment_metrics
    create_deployment_summary
    send_notification
}
```

#### PowerShell éƒ¨ç½²è„šæœ¬

```powershell
# docker-deploy.ps1

param(
    [string]$Action = "up",
    [switch]$Build,
    [switch]$Detached,
    [switch]$RemoveVolumes,
    [switch]$SkipChecks,
    [switch]$RollbackTag,
    [string]$Environment = "development",
    [switch]$Help
)

$SCRIPT_VERSION = "2.0.0"
$LOG_FILE = "C:\logs\fitness-gym-deploy.log"
$DEPLOYMENT_BACKUP_DIR = "C:\backups\fitness-gym"

function Test-Environment {
    Write-Host "ğŸ” éªŒè¯ç¯å¢ƒ..."
    Test-SystemResources
    Test-PortAvailability
    Test-EnvironmentVariables
}

function Invoke-PreDeploymentChecks {
    Write-Host "ğŸ›¡ï¸  æ‰§è¡Œé¢„éƒ¨ç½²æ£€æŸ¥..."
    New-DeploymentBackup
    Backup-Database
    Stop-ServicesGracefully
}

function Invoke-Deployment {
    Write-Host "ğŸš€ æ‰§è¡Œéƒ¨ç½²..."
    Update-ContainerImages
    Update-Configuration
    Start-Services
    Invoke-DatabaseMigrations
}

function Invoke-HealthChecks {
    Write-Host "ğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥..."
    Wait-Services
    Test-HealthChecks
    Test-Deployment
}

function Invoke-Rollback {
    param([string]$Tag)
    Write-Host "âª æ‰§è¡Œå›æ»šåˆ° $Tag..."
    Stop-CurrentServices
    Restore-FromBackup -Tag $Tag
    Start-PreviousVersion -Tag $Tag
    Test-HealthChecks
}
```

### 2.2 å¤šç¯å¢ƒéƒ¨ç½²ç­–ç•¥

#### ç¯å¢ƒé…ç½®ç®¡ç†

```yaml
# config/environments.yml
environments:
  development:
    replicas: 1
    resources:
      cpu: "0.5"
      memory: "512Mi"
    databases:
      - name: fitness_gym_dev
        backup_schedule: "0 */6 * * *"

  staging:
    replicas: 2
    resources:
      cpu: "1"
      memory: "1Gi"
    databases:
      - name: fitness_gym_staging
        backup_schedule: "0 */4 * * *"

  production:
    replicas: 3
    resources:
      cpu: "2"
      memory: "2Gi"
    databases:
      - name: fitness_gym_prod
        backup_schedule: "0 */2 * * *"
```

#### è“ç»¿éƒ¨ç½²å®ç°

```bash
#!/bin/bash
# scripts/blue-green-deploy.sh

BLUE_PORT=8080
GREEN_PORT=8081
CURRENT_COLOR=$(get_current_color)

if [ "$CURRENT_COLOR" = "blue" ]; then
    NEW_COLOR="green"
    OLD_PORT=$BLUE_PORT
    NEW_PORT=$GREEN_PORT
else
    NEW_COLOR="blue"
    OLD_PORT=$GREEN_PORT
    NEW_PORT=$BLUE_PORT
fi

echo "ğŸš€ å¯åŠ¨ $NEW_COLOR ç¯å¢ƒ..."
docker-compose -f docker-compose.$NEW_COLOR.yml up -d

echo "ğŸ¥ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
wait_for_service $NEW_PORT

echo "ğŸ§ª è¿è¡Œå†’çƒŸæµ‹è¯•..."
run_smoke_tests $NEW_PORT

if [ $? -eq 0 ]; then
    echo "âœ… æµ‹è¯•é€šè¿‡ï¼Œåˆ‡æ¢æµé‡..."
    switch_traffic $NEW_COLOR
    echo "ğŸ§¹ æ¸…ç†æ—§ç¯å¢ƒ..."
    docker-compose -f docker-compose.$CURRENT_COLOR.yml down
    update_current_color $NEW_COLOR
else
    echo "âŒ æµ‹è¯•å¤±è´¥ï¼Œå›æ»š..."
    docker-compose -f docker-compose.$NEW_COLOR.yml down
fi
```

### 2.3 éƒ¨ç½²éªŒè¯å’Œå›æ»š

#### éƒ¨ç½²åéªŒè¯

```bash
#!/bin/bash
# scripts/validate-deployment.sh

SERVICES=("api" "frontend" "database")
ENDPOINTS=(
    "http://localhost:3000/health"
    "http://localhost:8080/api/health"
    "postgres://user:pass@localhost:5432/fitness_gym"
)

validate_services() {
    for service in "${SERVICES[@]}"; do
        if ! docker-compose ps $service | grep -q "Up"; then
            echo "âŒ æœåŠ¡ $service æœªè¿è¡Œ"
            return 1
        fi
    done
    echo "âœ… æ‰€æœ‰æœåŠ¡è¿è¡Œæ­£å¸¸"
}

validate_endpoints() {
    for endpoint in "${ENDPOINTS[@]}"; do
        if ! curl -f -s $endpoint > /dev/null; then
            echo "âŒ ç«¯ç‚¹ $endpoint æ— å“åº”"
            return 1
        fi
    done
    echo "âœ… æ‰€æœ‰ç«¯ç‚¹å“åº”æ­£å¸¸"
}

validate_database() {
    if ! docker-compose exec -T database pg_isready -U fitness_user -d fitness_gym; then
        echo "âŒ æ•°æ®åº“è¿æ¥å¤±è´¥"
        return 1
    fi
    echo "âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸"
}

# æ‰§è¡ŒéªŒè¯
validate_services && validate_endpoints && validate_database
```

---

## 3. ç›‘æ§å’Œå‘Šè­¦è‡ªåŠ¨åŒ–

### 3.1 Prometheus + Grafana æ ˆ

#### Prometheus é…ç½®

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'fitness-gym-api'
    static_configs:
      - targets: ['api:3000']
    metrics_path: '/metrics'

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']
```

#### Alertmanager é…ç½®

```yaml
# monitoring/alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@fitness-gym.com'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'team-notifications'

receivers:
- name: 'team-notifications'
  email_configs:
  - to: 'ops@fitness-gym.com'
    send_resolved: true
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/.../.../...'
    channel: '#alerts'
    send_resolved: true
```

### 3.2 è‡ªå®šä¹‰ç›‘æ§æŒ‡æ ‡

#### åº”ç”¨æŒ‡æ ‡æ”¶é›†

```javascript
// api/src/metrics.js
const promClient = require('prom-client');

// åˆ›å»ºæ³¨å†Œå™¨
const register = new promClient.Registry();

// æ·»åŠ é»˜è®¤æŒ‡æ ‡
promClient.collectDefaultMetrics({ register });

// è‡ªå®šä¹‰æŒ‡æ ‡
const httpRequestDuration = new promClient.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code'],
  buckets: [0.1, 0.5, 1, 2, 5, 10]
});

const activeUsers = new promClient.Gauge({
  name: 'fitness_gym_active_users',
  help: 'Number of currently active users'
});

const workoutBookings = new promClient.Counter({
  name: 'fitness_gym_workout_bookings_total',
  help: 'Total number of workout bookings'
});

// ä¸­é—´ä»¶
app.use((req, res, next) => {
  const start = Date.now();
  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    httpRequestDuration
      .labels(req.method, req.route?.path || req.path, res.statusCode)
      .observe(duration);
  });
  next();
});

// æŒ‡æ ‡ç«¯ç‚¹
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});
```

#### ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§

```javascript
// ä¸šåŠ¡æŒ‡æ ‡æ›´æ–°
app.post('/api/workouts/:id/book', (req, res) => {
  // é¢„è®¢é€»è¾‘...
  workoutBookings.inc();
  activeUsers.inc();

  // å…¶ä»–ä¸šåŠ¡é€»è¾‘...
});
```

### 3.3 å‘Šè­¦è§„åˆ™å’Œå“åº”

#### æ™ºèƒ½å‘Šè­¦è§„åˆ™

```yaml
# monitoring/alerts.yml
groups:
- name: fitness_gym_alerts
  rules:
  # åº”ç”¨æ€§èƒ½å‘Šè­¦
  - alert: APISlowResponse
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "API å“åº”æ—¶é—´è¿‡æ…¢"
      description: "95th percentile å“åº”æ—¶é—´è¶…è¿‡ 2 ç§’"
      runbook_url: "https://docs.fitness-gym.com/runbooks/api-slow-response"

  # ç³»ç»Ÿèµ„æºå‘Šè­¦
  - alert: HighCPUUsage
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "CPU ä½¿ç”¨ç‡è¿‡é«˜"
      description: "CPU ä½¿ç”¨ç‡è¶…è¿‡ 85%"

  # ä¸šåŠ¡æŒ‡æ ‡å‘Šè­¦
  - alert: LowActiveUsers
    expr: fitness_gym_active_users < 10
    for: 15m
    labels:
      severity: info
    annotations:
      summary: "æ´»è·ƒç”¨æˆ·æ•°è¿‡ä½"
      description: "å½“å‰æ´»è·ƒç”¨æˆ·å°‘äº 10 äºº"

  # æœåŠ¡å¯ç”¨æ€§å‘Šè­¦
  - alert: ServiceDown
    expr: up == 0
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "æœåŠ¡ä¸å¯ç”¨"
      description: "æœåŠ¡ {{ $labels.job }} å·²ç»å®•æœºè¶…è¿‡ 2 åˆ†é’Ÿ"
```

### 3.4 è‡ªåŠ¨åŒ–å‘Šè­¦å“åº”

#### å‘Šè­¦å“åº”è„šæœ¬

```bash
#!/bin/bash
# monitoring/scripts/alert-responder.sh

ALERT_TYPE=$1
INSTANCE=$2

case $ALERT_TYPE in
    "HighCPUUsage")
        echo "ğŸ”¥ CPU ä½¿ç”¨ç‡è¿‡é«˜ï¼Œæ‰§è¡Œè‡ªåŠ¨æ‰©å®¹..."
        docker-compose up -d --scale api=2
        ;;
    "ServiceDown")
        echo "ğŸ’€ æœåŠ¡å®•æœºï¼Œæ‰§è¡Œè‡ªåŠ¨é‡å¯..."
        docker-compose restart $INSTANCE
        ;;
    "DatabaseConnectionError")
        echo "ğŸ”Œ æ•°æ®åº“è¿æ¥é”™è¯¯ï¼Œæ£€æŸ¥æ•°æ®åº“çŠ¶æ€..."
        docker-compose exec database pg_isready
        if [ $? -ne 0 ]; then
            docker-compose restart database
        fi
        ;;
    *)
        echo "âš ï¸ æœªçŸ¥å‘Šè­¦ç±»å‹: $ALERT_TYPE"
        ;;
esac

# å‘é€å“åº”é€šçŸ¥
curl -X POST -H 'Content-type: application/json' \
    --data "{\"text\":\"è‡ªåŠ¨å“åº”å®Œæˆ: $ALERT_TYPE\"}" \
    $SLACK_WEBHOOK_URL
```

---

## 4. å¤‡ä»½æ¢å¤è‡ªåŠ¨åŒ–

### 4.1 æ•°æ®åº“å¤‡ä»½ç­–ç•¥

#### è‡ªåŠ¨åŒ–å¤‡ä»½è„šæœ¬

```powershell
# scripts/backup/backup-database.ps1

param(
    [ValidateSet("full", "incremental", "differential")]
    [string]$BackupType = "full",
    [switch]$Compress,
    [switch]$Encrypt,
    [string]$EncryptionKey,
    [switch]$UploadToS3,
    [string]$S3Bucket,
    [string]$S3Path
)

$BACKUP_DIR = "C:\backups\fitness-gym"
$TIMESTAMP = Get-Date -Format "yyyyMMdd_HHmmss"
$BACKUP_FILE = "$BACKUP_DIR\fitness_gym_db_backup_$BackupType_$TIMESTAMP.sql"

# åˆ›å»ºå¤‡ä»½ç›®å½•
if (!(Test-Path $BACKUP_DIR)) {
    New-Item -ItemType Directory -Path $BACKUP_DIR -Force
}

# æ‰§è¡Œå¤‡ä»½
switch ($BackupType) {
    "full" {
        Write-Host "ğŸ“¦ æ‰§è¡Œå®Œæ•´å¤‡ä»½..."
        pg_dump -U fitness_user -h localhost -d fitness_gym -f $BACKUP_FILE
    }
    "incremental" {
        Write-Host "ğŸ”„ æ‰§è¡Œå¢é‡å¤‡ä»½..."
        # å¢é‡å¤‡ä»½é€»è¾‘
        pg_dump -U fitness_user -h localhost -d fitness_gym --format=directory --compress=9 --file=$BACKUP_FILE
    }
}

# å‹ç¼©å¤‡ä»½
if ($Compress) {
    Write-Host "ğŸ—œï¸ å‹ç¼©å¤‡ä»½æ–‡ä»¶..."
    & "C:\Program Files\7-Zip\7z.exe" a "$BACKUP_FILE.7z" $BACKUP_FILE
    Remove-Item $BACKUP_FILE
    $BACKUP_FILE = "$BACKUP_FILE.7z"
}

# åŠ å¯†å¤‡ä»½
if ($Encrypt) {
    Write-Host "ğŸ” åŠ å¯†å¤‡ä»½æ–‡ä»¶..."
    # ä½¿ç”¨ AES åŠ å¯†
    $encryptedFile = "$BACKUP_FILE.aes"
    # åŠ å¯†é€»è¾‘...
}

# ä¸Šä¼ åˆ° S3
if ($UploadToS3) {
    Write-Host "â˜ï¸ ä¸Šä¼ åˆ° S3..."
    & aws s3 cp $BACKUP_FILE s3://$S3Bucket/$S3Path/ --recursive
}

# æ¸…ç†æ—§å¤‡ä»½
Write-Host "ğŸ§¹ æ¸…ç†æ—§å¤‡ä»½..."
Get-ChildItem $BACKUP_DIR | Where-Object { $_.LastWriteTime -lt (Get-Date).AddDays(-30) } | Remove-Item

Write-Host "âœ… å¤‡ä»½å®Œæˆ: $BACKUP_FILE"
```

#### å¤‡ä»½éªŒè¯è„šæœ¬

```powershell
# scripts/backup/verify-backup.ps1

param(
    [string]$BackupFile,
    [switch]$RestoreTest,
    [string]$TestDatabase = "fitness_gym_test_restore"
)

Write-Host "ğŸ” éªŒè¯å¤‡ä»½æ–‡ä»¶..."

# æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
if (!(Test-Path $BackupFile)) {
    Write-Error "å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $BackupFile"
    exit 1
}

# è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
$hash = Get-FileHash $BackupFile -Algorithm SHA256
Write-Host "æ–‡ä»¶å“ˆå¸Œ: $($hash.Hash)"

# æµ‹è¯•æ¢å¤ï¼ˆå¯é€‰ï¼‰
if ($RestoreTest) {
    Write-Host "ğŸ§ª æ‰§è¡Œæ¢å¤æµ‹è¯•..."

    # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
    psql -U postgres -c "CREATE DATABASE $TestDatabase;"

    # æ¢å¤åˆ°æµ‹è¯•æ•°æ®åº“
    if ($BackupFile -like "*.7z") {
        # è§£å‹å¹¶æ¢å¤
        & "C:\Program Files\7-Zip\7z.exe" x $BackupFile -o"$env:TEMP\backup_extract"
        pg_restore -U fitness_user -d $TestDatabase "$env:TEMP\backup_extract"
    } else {
        pg_restore -U fitness_user -d $TestDatabase $BackupFile
    }

    # éªŒè¯æ•°æ®å®Œæ•´æ€§
    $recordCount = psql -U fitness_user -d $TestDatabase -c "SELECT COUNT(*) FROM users;" -t
    Write-Host "æ¢å¤çš„è®°å½•æ•°: $recordCount"

    # æ¸…ç†æµ‹è¯•æ•°æ®åº“
    psql -U postgres -c "DROP DATABASE $TestDatabase;"
}

Write-Host "âœ… å¤‡ä»½éªŒè¯å®Œæˆ"
```

### 4.2 å¤‡ä»½ç›‘æ§å’Œå‘Šè­¦

#### å¤‡ä»½çŠ¶æ€ç›‘æ§

```powershell
# scripts/backup/backup-monitor.ps1

param(
    [string]$BackupDir = "C:\backups\fitness-gym",
    [switch]$GenerateReport,
    [switch]$SendNotifications,
    [string]$SlackWebhookUrl
)

$BACKUP_THRESHOLD_HOURS = 25  # å¤‡ä»½é—´éš”é˜ˆå€¼ï¼ˆå°æ—¶ï¼‰
$MIN_BACKUP_SIZE_MB = 10     # æœ€å°å¤‡ä»½å¤§å°ï¼ˆMBï¼‰

Write-Host "ğŸ“Š ç›‘æ§å¤‡ä»½çŠ¶æ€..."

# æ£€æŸ¥æœ€æ–°å¤‡ä»½
$latestBackup = Get-ChildItem $BackupDir -File | Sort-Object LastWriteTime -Descending | Select-Object -First 1

if (!$latestBackup) {
    Write-Warning "âŒ æœªæ‰¾åˆ°å¤‡ä»½æ–‡ä»¶"
    $status = "CRITICAL"
} else {
    $hoursSinceBackup = (Get-Date) - $latestBackup.LastWriteTime).TotalHours
    $backupSizeMB = [math]::Round($latestBackup.Length / 1MB, 2)

    if ($hoursSinceBackup -gt $BACKUP_THRESHOLD_HOURS) {
        Write-Warning "âš ï¸  å¤‡ä»½å¤ªæ—§: $([math]::Round($hoursSinceBackup, 1)) å°æ—¶å‰"
        $status = "WARNING"
    } elseif ($backupSizeMB -lt $MIN_BACKUP_SIZE_MB) {
        Write-Warning "âš ï¸  å¤‡ä»½æ–‡ä»¶å¤ªå°: $backupSizeMB MB"
        $status = "WARNING"
    } else {
        Write-Host "âœ… å¤‡ä»½æ­£å¸¸: $backupSizeMB MB, $([math]::Round($hoursSinceBackup, 1)) å°æ—¶å‰"
        $status = "OK"
    }
}

# ç”ŸæˆæŠ¥å‘Š
if ($GenerateReport) {
    $report = @{
        timestamp = Get-Date
        status = $status
        latest_backup = $latestBackup.Name
        backup_age_hours = $hoursSinceBackup
        backup_size_mb = $backupSizeMB
        backup_location = $BackupDir
    }

    $report | ConvertTo-Json | Out-File "backup-status-report.json"
    Write-Host "ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: backup-status-report.json"
}

# å‘é€é€šçŸ¥
if ($SendNotifications -and $status -ne "OK") {
    $message = @{
        text = "å¤‡ä»½çŠ¶æ€å‘Šè­¦: $status`næœ€æ–°å¤‡ä»½: $($latestBackup.Name)`nå¤‡ä»½æ—¶é—´: $($latestBackup.LastWriteTime)`nçŠ¶æ€è¯¦æƒ…: $(if ($status -eq 'CRITICAL') { 'æ— å¤‡ä»½æ–‡ä»¶' } else { "$([math]::Round($hoursSinceBackup, 1)) å°æ—¶å‰, $backupSizeMB MB" })"
    }

    Invoke-RestMethod -Uri $SlackWebhookUrl -Method Post -Body ($message | ConvertTo-Json) -ContentType 'application/json'
}

Write-Host "ğŸ å¤‡ä»½ç›‘æ§å®Œæˆ"
```

### 4.3 ç¾éš¾æ¢å¤æ¼”ç»ƒ

#### æ¢å¤æ¼”ç»ƒè„šæœ¬

```bash
#!/bin/bash
# scripts/disaster-recovery/dr-drill.sh

LOG_FILE="/var/log/dr-drill.log"
DRILL_DATE=$(date +%Y%m%d_%H%M%S)
DRILL_TAG="dr-drill-$DRILL_DATE"

echo "ğŸ§ª å¼€å§‹ç¾éš¾æ¢å¤æ¼”ç»ƒ - $DRILL_DATE" | tee -a $LOG_FILE

# è®°å½•å¼€å§‹æ—¶é—´
START_TIME=$(date +%s)

# æ­¥éª¤ 1: æ¨¡æ‹Ÿæ•…éšœ
echo "ğŸ’¥ æ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒæ•…éšœ..." | tee -a $LOG_FILE
simulate_production_failure

# æ­¥éª¤ 2: æ¿€æ´»ç¾éš¾æ¢å¤
echo "ğŸš¨ æ¿€æ´»ç¾éš¾æ¢å¤æµç¨‹..." | tee -a $LOG_FILE
activate_disaster_recovery

# æ­¥éª¤ 3: æ¢å¤æ•°æ®
echo "ğŸ’¾ ä»å¤‡ä»½æ¢å¤æ•°æ®..." | tee -a $LOG_FILE
restore_from_backup

# æ­¥éª¤ 4: é‡å»ºæœåŠ¡
echo "ğŸ”¨ é‡å»ºæœåŠ¡æ ˆ..." | tee -a $LOG_FILE
rebuild_services

# æ­¥éª¤ 5: éªŒè¯æ¢å¤
echo "âœ… éªŒè¯ç³»ç»Ÿæ¢å¤..." | tee -a $LOG_FILE
validate_recovery

# è®¡ç®—æ¢å¤æ—¶é—´
END_TIME=$(date +%s)
RECOVERY_TIME=$((END_TIME - START_TIME))

echo "â±ï¸  æ¢å¤æ—¶é—´: $RECOVERY_TIME ç§’" | tee -a $LOG_FILE

# ç”Ÿæˆæ¼”ç»ƒæŠ¥å‘Š
generate_drill_report $RECOVERY_TIME $DRILL_TAG

echo "ğŸ“Š ç¾éš¾æ¢å¤æ¼”ç»ƒå®Œæˆ - æŸ¥çœ‹æŠ¥å‘Š: dr-drill-report-$DRILL_TAG.json" | tee -a $LOG_FILE
```

---

## 5. åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–

### 5.1 Docker å®¹å™¨ç¼–æ’

#### Docker Compose é…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    environment:
      - NODE_ENV=production
      - DATABASE_URL=postgresql://user:pass@database:5432/fitness_gym
    ports:
      - "3000:3000"
    depends_on:
      - database
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "8080:80"
    depends_on:
      - api
    restart: unless-stopped

  database:
    image: postgres:14
    environment:
      - POSTGRES_DB=fitness_gym
      - POSTGRES_USER=fitness_user
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U fitness_user -d fitness_gym"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
```

### 5.2 å®¹å™¨å¥åº·æ£€æŸ¥

#### è‡ªå®šä¹‰å¥åº·æ£€æŸ¥è„šæœ¬

```bash
#!/bin/bash
# monitoring/scripts/health-check.sh

# API å¥åº·æ£€æŸ¥
check_api() {
    if curl -f -s --max-time 10 http://localhost:3000/health > /dev/null; then
        echo "âœ… API æœåŠ¡æ­£å¸¸"
        return 0
    else
        echo "âŒ API æœåŠ¡å¼‚å¸¸"
        return 1
    fi
}

# æ•°æ®åº“å¥åº·æ£€æŸ¥
check_database() {
    if docker-compose exec -T database pg_isready -U fitness_user -d fitness_gym > /dev/null; then
        echo "âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸"
        return 0
    else
        echo "âŒ æ•°æ®åº“è¿æ¥å¼‚å¸¸"
        return 1
    fi
}

# å‰ç«¯å¥åº·æ£€æŸ¥
check_frontend() {
    if curl -f -s --max-time 10 http://localhost:8080 > /dev/null; then
        echo "âœ… å‰ç«¯æœåŠ¡æ­£å¸¸"
        return 0
    else
        echo "âŒ å‰ç«¯æœåŠ¡å¼‚å¸¸"
        return 1
    fi
}

# Redis å¥åº·æ£€æŸ¥
check_redis() {
    if docker-compose exec -T redis redis-cli ping | grep -q PONG; then
        echo "âœ… Redis æœåŠ¡æ­£å¸¸"
        return 0
    else
        echo "âŒ Redis æœåŠ¡å¼‚å¸¸"
        return 1
    fi
}

# è¿è¡Œæ‰€æœ‰æ£€æŸ¥
run_all_checks() {
    local failed_checks=()

    check_api || failed_checks+=("API")
    check_database || failed_checks+=("Database")
    check_frontend || failed_checks+=("Frontend")
    check_redis || failed_checks+=("Redis")

    if [ ${#failed_checks[@]} -eq 0 ]; then
        echo "ğŸ‰ æ‰€æœ‰æœåŠ¡è¿è¡Œæ­£å¸¸"
        return 0
    else
        echo "âš ï¸  ä»¥ä¸‹æœåŠ¡å¼‚å¸¸: ${failed_checks[*]}"
        return 1
    fi
}

# ä¸»å‡½æ•°
main() {
    case "${1:-all}" in
        "api") check_api ;;
        "database") check_database ;;
        "frontend") check_frontend ;;
        "redis") check_redis ;;
        "all") run_all_checks ;;
        *) echo "ç”¨æ³•: $0 [api|database|frontend|redis|all]"; exit 1 ;;
    esac
}

main "$@"
```

### 5.3 è‡ªåŠ¨æ‰©ç¼©å®¹

#### åŸºäºæŒ‡æ ‡çš„è‡ªåŠ¨æ‰©ç¼©å®¹

```bash
#!/bin/bash
# scripts/auto-scaling.sh

CPU_THRESHOLD=80
MEMORY_THRESHOLD=85
MAX_INSTANCES=5
MIN_INSTANCES=1

while true; do
    # è·å–å½“å‰å®ä¾‹æ•°
    CURRENT_INSTANCES=$(docker-compose ps api | grep -c "Up")

    # è·å– CPU ä½¿ç”¨ç‡
    CPU_USAGE=$(docker stats --no-stream --format "table {{.CPUPerc}}" | tail -n +2 | sed 's/%//' | awk '{sum+=$1} END {print sum/NR}')

    # è·å–å†…å­˜ä½¿ç”¨ç‡
    MEMORY_USAGE=$(docker stats --no-stream --format "table {{.MemPerc}}" | tail -n +2 | sed 's/%//' | awk '{sum+=$1} END {print sum/NR}')

    echo "$(date): CPU: ${CPU_USAGE}%, Memory: ${MEMORY_USAGE}%, Instances: $CURRENT_INSTANCES"

    # æ‰©å®¹æ¡ä»¶
    if (( $(echo "$CPU_USAGE > $CPU_THRESHOLD" | bc -l) )) || (( $(echo "$MEMORY_USAGE > $MEMORY_THRESHOLD" | bc -l) )); then
        if [ $CURRENT_INSTANCES -lt $MAX_INSTANCES ]; then
            NEW_INSTANCES=$((CURRENT_INSTANCES + 1))
            echo "ğŸ“ˆ æ‰©å®¹åˆ° $NEW_INSTANCES ä¸ªå®ä¾‹"
            docker-compose up -d --scale api=$NEW_INSTANCES
        fi

    # ç¼©å®¹æ¡ä»¶
    elif (( $(echo "$CPU_USAGE < 30" | bc -l) )) && (( $(echo "$MEMORY_USAGE < 40" | bc -l) )); then
        if [ $CURRENT_INSTANCES -gt $MIN_INSTANCES ]; then
            NEW_INSTANCES=$((CURRENT_INSTANCES - 1))
            echo "ğŸ“‰ ç¼©å®¹åˆ° $NEW_INSTANCES ä¸ªå®ä¾‹"
            docker-compose up -d --scale api=$NEW_INSTANCES
        fi
    fi

    sleep 60  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
done
```

---

## 6. å®‰å…¨è‡ªåŠ¨åŒ–

### 6.1 è‡ªåŠ¨åŒ–å®‰å…¨æ‰«æ

#### å®¹å™¨é•œåƒå®‰å…¨æ‰«æ

```yaml
# .github/workflows/security-scan.yml
name: Security Scan

on:
  push:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * *'  # æ¯æ—¥å‡Œæ™¨2ç‚¹

jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Build Docker images
        run: |
          docker build -t fitness-gym-api ./api
          docker build -t fitness-gym-frontend ./frontend

      - name: Scan API image
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'image'
          scan-ref: 'fitness-gym-api'
          format: 'sarif'
          output: 'trivy-api-results.sarif'

      - name: Scan Frontend image
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'image'
          scan-ref: 'fitness-gym-frontend'
          format: 'sarif'
          output: 'trivy-frontend-results.sarif'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-api-results.sarif'

      - name: Upload Frontend scan results
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-frontend-results.sarif'

      - name: Dependency check
        uses: dependency-check/Dependency-Check_Action@main
        with:
          project: 'Fitness Gym'
          path: '.'
          format: 'ALL'
```

#### ä»£ç å®‰å…¨æ‰«æ

```yaml
# .github/workflows/code-security.yml
name: Code Security

on: [push, pull_request]

jobs:
  code-security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run Snyk to check for vulnerabilities
        uses: snyk/actions/node@master
        continue-on-error: true
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          args: --file=package.json

      - name: CodeQL Analysis
        uses: github/codeql-action/init@v2
        with:
          languages: javascript, python

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v2
```

### 6.2 è‡ªåŠ¨åŒ–å¯†é’¥ç®¡ç†

#### å¯†é’¥è½®æ¢è„šæœ¬

```bash
#!/bin/bash
# scripts/security/rotate-secrets.sh

# JWT å¯†é’¥è½®æ¢
rotate_jwt_secrets() {
    echo "ğŸ”„ è½®æ¢ JWT å¯†é’¥..."

    # ç”Ÿæˆæ–°å¯†é’¥
    NEW_JWT_SECRET=$(openssl rand -hex 32)
    NEW_JWT_REFRESH_SECRET=$(openssl rand -hex 32)

    # æ›´æ–°ç¯å¢ƒå˜é‡
    sed -i "s/JWT_SECRET=.*/JWT_SECRET=$NEW_JWT_SECRET/" .env
    sed -i "s/JWT_REFRESH_SECRET=.*/JWT_REFRESH_SECRET=$NEW_JWT_REFRESH_SECRET/" .env

    # é‡å¯æœåŠ¡
    docker-compose restart api

    echo "âœ… JWT å¯†é’¥è½®æ¢å®Œæˆ"
}

# æ•°æ®åº“å¯†ç è½®æ¢
rotate_database_password() {
    echo "ğŸ”„ è½®æ¢æ•°æ®åº“å¯†ç ..."

    # ç”Ÿæˆæ–°å¯†ç 
    NEW_DB_PASSWORD=$(openssl rand -base64 16)

    # æ›´æ–° PostgreSQL å¯†ç 
    docker-compose exec database psql -U postgres -c "ALTER USER fitness_user PASSWORD '$NEW_DB_PASSWORD';"

    # æ›´æ–°ç¯å¢ƒå˜é‡
    sed -i "s/DB_PASSWORD=.*/DB_PASSWORD=$NEW_DB_PASSWORD/" .env

    # é‡å¯æœåŠ¡
    docker-compose restart api

    echo "âœ… æ•°æ®åº“å¯†ç è½®æ¢å®Œæˆ"
}

# API Key è½®æ¢
rotate_api_keys() {
    echo "ğŸ”„ è½®æ¢ API å¯†é’¥..."

    # ç”Ÿæˆæ–°å¯†é’¥
    NEW_STRIPE_SECRET=$(openssl rand -hex 32)
    NEW_SENDGRID_API_KEY=$(openssl rand -hex 32)

    # æ›´æ–°ç¯å¢ƒå˜é‡
    sed -i "s/STRIPE_SECRET_KEY=.*/STRIPE_SECRET_KEY=$NEW_STRIPE_SECRET/" .env
    sed -i "s/SENDGRID_API_KEY=.*/SENDGRID_API_KEY=$NEW_SENDGRID_API_KEY/" .env

    # é‡å¯æœåŠ¡
    docker-compose restart api

    echo "âœ… API å¯†é’¥è½®æ¢å®Œæˆ"
}

# ä¸»å‡½æ•°
main() {
    case "${1:-all}" in
        "jwt") rotate_jwt_secrets ;;
        "database") rotate_database_password ;;
        "api-keys") rotate_api_keys ;;
        "all")
            rotate_jwt_secrets
            rotate_database_password
            rotate_api_keys
            ;;
        *) echo "ç”¨æ³•: $0 [jwt|database|api-keys|all]"; exit 1 ;;
    esac
}

main "$@"
```

---

## 7. æ–‡æ¡£è‡ªåŠ¨åŒ–

### 7.1 æ–‡æ¡£ç”Ÿæˆå·¥å…·

#### API æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ

```javascript
// docs/scripts/generate-api-docs.js

const swaggerJsdoc = require('swagger-jsdoc');
const fs = require('fs');
const path = require('path');

const options = {
  definition: {
    openapi: '3.0.0',
    info: {
      title: 'Fitness Gym API',
      version: '1.0.0',
      description: 'å¥èº«æˆ¿ç®¡ç†ç³»ç»Ÿ API æ–‡æ¡£',
    },
    servers: [
      {
        url: 'http://localhost:3000',
        description: 'å¼€å‘ç¯å¢ƒ',
      },
      {
        url: 'https://api.fitness-gym.com',
        description: 'ç”Ÿäº§ç¯å¢ƒ',
      },
    ],
  },
  apis: ['./api/routes/*.js', './api/models/*.js'], // API è·¯ç”±å’Œæ¨¡å‹æ–‡ä»¶
};

const specs = swaggerJsdoc(options);

// ç”Ÿæˆ Markdown æ ¼å¼æ–‡æ¡£
function generateMarkdown(specs) {
  let markdown = '# Fitness Gym API æ–‡æ¡£\n\n';

  // éå†æ‰€æœ‰ç«¯ç‚¹
  Object.keys(specs.paths).forEach(path => {
    Object.keys(specs.paths[path]).forEach(method => {
      const endpoint = specs.paths[path][method];
      markdown += `## ${method.toUpperCase()} ${path}\n\n`;
      markdown += `**æè¿°**: ${endpoint.summary || 'æ— æè¿°'}\n\n`;

      if (endpoint.parameters) {
        markdown += '**å‚æ•°**:\n\n';
        endpoint.parameters.forEach(param => {
          markdown += `- \`${param.name}\` (${param.in}): ${param.description || 'æ— æè¿°'}\n`;
        });
        markdown += '\n';
      }

      if (endpoint.responses) {
        markdown += '**å“åº”**:\n\n';
        Object.keys(endpoint.responses).forEach(code => {
          const response = endpoint.responses[code];
          markdown += `- \`${code}\`: ${response.description || 'æ— æè¿°'}\n`;
        });
        markdown += '\n';
      }

      markdown += '---\n\n';
    });
  });

  return markdown;
}

const markdownDocs = generateMarkdown(specs);
fs.writeFileSync('./docs/api/API_DOCUMENTATION.md', markdownDocs);

console.log('âœ… API æ–‡æ¡£ç”Ÿæˆå®Œæˆ');
```

#### éƒ¨ç½²æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ

```javascript
// docs/scripts/generate-deployment-docs.js

const fs = require('fs');
const path = require('path');
const { execSync } = require('child_process');

function generateDeploymentDocs() {
  const docs = {};

  // è·å– Docker é•œåƒä¿¡æ¯
  try {
    const images = execSync('docker images --format "table {{.Repository}}:{{.Tag}}\\t{{.Size}}\\t{{.CreatedAt}}"', { encoding: 'utf8' });
    docs.dockerImages = images;
  } catch (error) {
    docs.dockerImages = 'æ— æ³•è·å– Docker é•œåƒä¿¡æ¯';
  }

  // è·å–ç¯å¢ƒå˜é‡
  const envPath = path.join(__dirname, '../../.env');
  if (fs.existsSync(envPath)) {
    const envContent = fs.readFileSync(envPath, 'utf8');
    const envVars = envContent.split('\n')
      .filter(line => line.includes('='))
      .map(line => line.split('=')[0])
      .join(', ');
    docs.environmentVariables = envVars;
  }

  // è·å–æœ€è¿‘çš„éƒ¨ç½²å†å²
  try {
    const gitLog = execSync('git log --oneline -10 --grep="deploy"', { encoding: 'utf8' });
    docs.deploymentHistory = gitLog;
  } catch (error) {
    docs.deploymentHistory = 'æ— æ³•è·å–éƒ¨ç½²å†å²';
  }

  // ç”Ÿæˆéƒ¨ç½²æ–‡æ¡£
  const deploymentDoc = `# éƒ¨ç½²é…ç½®æ–‡æ¡£

## Docker é•œåƒä¿¡æ¯

\`\`\`
${docs.dockerImages}
\`\`\`

## ç¯å¢ƒå˜é‡

é…ç½®çš„ç¯å¢ƒå˜é‡: ${docs.environmentVariables}

## æœ€è¿‘éƒ¨ç½²å†å²

\`\`\`
${docs.deploymentHistory}
\`\`\`

*ç”Ÿæˆæ—¶é—´: ${new Date().toISOString()}*
`;

  fs.writeFileSync('./docs/deployment/DEPLOYMENT_CONFIG.md', deploymentDoc);
  console.log('âœ… éƒ¨ç½²æ–‡æ¡£ç”Ÿæˆå®Œæˆ');
}

generateDeploymentDocs();
```

### 7.2 æ–‡æ¡£éªŒè¯å’Œå‘å¸ƒ

#### æ–‡æ¡£éªŒè¯è„šæœ¬

```javascript
// docs/scripts/validate-docs.js

const fs = require('fs');
const path = require('path');
const markdownLinkCheck = require('markdown-link-check');

const DOCS_DIR = './docs';

function validateMarkdownFile(filePath) {
  return new Promise((resolve, reject) => {
    fs.readFile(filePath, 'utf8', (err, content) => {
      if (err) {
        reject(err);
        return;
      }

      const errors = [];

      // æ£€æŸ¥å¿…éœ€çš„ç« èŠ‚
      const requiredSections = ['æ¦‚è¿°', 'å®‰è£…', 'ä½¿ç”¨', 'é…ç½®'];
      requiredSections.forEach(section => {
        if (!content.includes(`# ${section}`) && !content.includes(`## ${section}`)) {
          errors.push(`ç¼ºå°‘å¿…éœ€ç« èŠ‚: ${section}`);
        }
      });

      // æ£€æŸ¥é“¾æ¥
      markdownLinkCheck(content, { baseUrl: 'file://' + path.dirname(filePath) }, (err, results) => {
        if (err) {
          errors.push(`é“¾æ¥æ£€æŸ¥é”™è¯¯: ${err.message}`);
        } else {
          results.forEach(result => {
            if (result.status === 'dead') {
              errors.push(`æ— æ•ˆé“¾æ¥: ${result.link}`);
            }
          });
        }

        resolve({ file: filePath, errors });
      });
    });
  });
}

async function validateAllDocs() {
  const markdownFiles = [];

  function findMarkdownFiles(dir) {
    const files = fs.readdirSync(dir);
    files.forEach(file => {
      const filePath = path.join(dir, file);
      const stat = fs.statSync(filePath);
      if (stat.isDirectory()) {
        findMarkdownFiles(filePath);
      } else if (file.endsWith('.md')) {
        markdownFiles.push(filePath);
      }
    });
  }

  findMarkdownFiles(DOCS_DIR);

  console.log(`ğŸ” éªŒè¯ ${markdownFiles.length} ä¸ªæ–‡æ¡£æ–‡ä»¶...`);

  const results = await Promise.all(markdownFiles.map(validateMarkdownFile));
  const totalErrors = results.reduce((sum, result) => sum + result.errors.length, 0);

  if (totalErrors === 0) {
    console.log('âœ… æ‰€æœ‰æ–‡æ¡£éªŒè¯é€šè¿‡');
  } else {
    console.log(`âŒ å‘ç° ${totalErrors} ä¸ªé—®é¢˜:`);
    results.forEach(result => {
      if (result.errors.length > 0) {
        console.log(`ğŸ“„ ${result.file}:`);
        result.errors.forEach(error => console.log(`  - ${error}`));
      }
    });
    process.exit(1);
  }
}

validateAllDocs();
```

#### æ–‡æ¡£å‘å¸ƒå·¥ä½œæµ

```yaml
# .github/workflows/docs-publish.yml
name: Publish Documentation

on:
  push:
    branches: [ main ]
    paths:
      - 'docs/**'
      - 'api/**'
      - 'docker-compose.yml'

jobs:
  publish-docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install dependencies
        run: npm install

      - name: Validate documentation
        run: npm run validate-docs

      - name: Generate API docs
        run: npm run generate-api-docs

      - name: Generate deployment docs
        run: npm run generate-deployment-docs

      - name: Update documentation index
        run: npm run update-doc-index

      - name: Build documentation site
        run: |
          cd docs
          npm install
          npm run build

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs/build
          cname: docs.fitness-gym.com
```

---

## 8. è¿ç»´æµç¨‹è‡ªåŠ¨åŒ–

### 8.1 å®šæœŸç»´æŠ¤ä»»åŠ¡

#### æ•°æ®åº“ç»´æŠ¤è„šæœ¬

```bash
#!/bin/bash
# scripts/maintenance/database-maintenance.sh

LOG_FILE="/var/log/database-maintenance.log"

echo "$(date): å¼€å§‹æ•°æ®åº“ç»´æŠ¤" >> $LOG_FILE

# åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
echo "$(date): åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯..." >> $LOG_FILE
docker-compose exec database psql -U fitness_user -d fitness_gym -c "ANALYZE VERBOSE;"

# æ¸…ç†è¿‡æœŸæ•°æ®
echo "$(date): æ¸…ç†è¿‡æœŸæ—¥å¿—..." >> $LOG_FILE
docker-compose exec database psql -U fitness_user -d fitness_gym -c "
DELETE FROM audit_logs WHERE created_at < NOW() - INTERVAL '90 days';
DELETE FROM session_logs WHERE created_at < NOW() - INTERVAL '30 days';
"

# é‡æ–°ç´¢å¼•
echo "$(date): é‡æ–°ç´¢å¼•å¤§è¡¨..." >> $LOG_FILE
docker-compose exec database psql -U fitness_user -d fitness_gym -c "
REINDEX TABLE CONCURRENTLY users;
REINDEX TABLE CONCURRENTLY workouts;
REINDEX TABLE CONCURRENTLY bookings;
"

# æ¸…ç†è¡¨ç©ºé—´
echo "$(date): æ¸…ç†è¡¨ç©ºé—´..." >> $LOG_FILE
docker-compose exec database psql -U fitness_user -d fitness_gym -c "VACUUM ANALYZE;"

echo "$(date): æ•°æ®åº“ç»´æŠ¤å®Œæˆ" >> $LOG_FILE
```

#### ç³»ç»Ÿæ¸…ç†è„šæœ¬

```bash
#!/bin/bash
# scripts/maintenance/system-cleanup.sh

LOG_FILE="/var/log/system-cleanup.log"

echo "$(date): å¼€å§‹ç³»ç»Ÿæ¸…ç†" >> $LOG_FILE

# æ¸…ç† Docker èµ„æº
echo "$(date): æ¸…ç†æœªä½¿ç”¨çš„ Docker èµ„æº..." >> $LOG_FILE
docker system prune -f
docker volume prune -f
docker image prune -f

# æ¸…ç†æ—¥å¿—æ–‡ä»¶
echo "$(date): æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶..." >> $LOG_FILE
find /var/log -name "*.log" -mtime +30 -exec gzip {} \;
find /var/log -name "*.gz" -mtime +90 -delete

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
echo "$(date): æ¸…ç†ä¸´æ—¶æ–‡ä»¶..." >> $LOG_FILE
find /tmp -type f -mtime +7 -delete
find /var/tmp -type f -mtime +7 -delete

# æ¸…ç†å¤‡ä»½æ–‡ä»¶ï¼ˆä¿ç•™æœ€è¿‘7å¤©çš„ï¼‰
echo "$(date): æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶..." >> $LOG_FILE
find /opt/fitness-gym/backups -name "*.sql" -mtime +7 -delete
find /opt/fitness-gym/backups -name "*.7z" -mtime +30 -delete

echo "$(date): ç³»ç»Ÿæ¸…ç†å®Œæˆ" >> $LOG_FILE
```

### 8.2 å®¹é‡è§„åˆ’è‡ªåŠ¨åŒ–

#### å®¹é‡ç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# scripts/monitoring/capacity-planning.sh

REPORT_FILE="/var/log/capacity-report.json"
THRESHOLD_CPU=80
THRESHOLD_MEMORY=85
THRESHOLD_DISK=90

# æ”¶é›†å½“å‰æŒ‡æ ‡
collect_metrics() {
    # CPU ä½¿ç”¨ç‡
    CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | sed "s/.*, *\([0-9.]*\)%* id.*/\1/" | awk '{print 100 - $1}')

    # å†…å­˜ä½¿ç”¨ç‡
    MEMORY_USAGE=$(free | grep Mem | awk '{printf "%.2f", $3/$2 * 100.0}')

    # ç£ç›˜ä½¿ç”¨ç‡
    DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')

    # æ•°æ®åº“è¿æ¥æ•°
    DB_CONNECTIONS=$(docker-compose exec -T database psql -U fitness_user -d fitness_gym -c "SELECT count(*) FROM pg_stat_activity;" -t | tr -d ' ')

    # API å“åº”æ—¶é—´ (P95)
    API_RESPONSE_TIME=$(curl -s -w "%{time_total}" -o /dev/null http://localhost:3000/health)

    echo "{
  \"timestamp\": \"$(date -Iseconds)\",
  \"cpu_usage_percent\": $CPU_USAGE,
  \"memory_usage_percent\": $MEMORY_USAGE,
  \"disk_usage_percent\": $DISK_USAGE,
  \"db_connections\": $DB_CONNECTIONS,
  \"api_response_time\": $API_RESPONSE_TIME
}"
}

# ç”Ÿæˆå®¹é‡å»ºè®®
generate_recommendations() {
    local metrics=$1

    echo "åŸºäºå½“å‰æŒ‡æ ‡çš„å®¹é‡å»ºè®®:"

    local cpu=$(echo $metrics | jq -r '.cpu_usage_percent')
    local memory=$(echo $metrics | jq -r '.memory_usage_percent')
    local disk=$(echo $metrics | jq -r '.disk_usage_percent')

    if (( $(echo "$cpu > $THRESHOLD_CPU" | bc -l) )); then
        echo "- CPU ä½¿ç”¨ç‡è¿‡é«˜ ($cpu%)ï¼Œå»ºè®®å¢åŠ  CPU æ ¸å¿ƒæˆ–ä¼˜åŒ–åº”ç”¨æ€§èƒ½"
    fi

    if (( $(echo "$memory > $THRESHOLD_MEMORY" | bc -l) )); then
        echo "- å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ($memory%)ï¼Œå»ºè®®å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨"
    fi

    if (( disk > THRESHOLD_DISK )); then
        echo "- ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜ ($disk%)ï¼Œå»ºè®®æ¸…ç†ç£ç›˜ç©ºé—´æˆ–å¢åŠ å­˜å‚¨"
    fi
}

# ä¸»å‡½æ•°
main() {
    echo "ğŸ“Š ç”Ÿæˆå®¹é‡è§„åˆ’æŠ¥å‘Š..."

    METRICS=$(collect_metrics)
    echo $METRICS > $REPORT_FILE

    echo "ğŸ“ˆ å®¹é‡å»ºè®®:"
    generate_recommendations "$METRICS"

    echo "âœ… å®¹é‡è§„åˆ’æŠ¥å‘Šå·²ç”Ÿæˆ: $REPORT_FILE"
}

main
```

### 8.3 è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ

#### æ¯æ—¥è¿ç»´æŠ¥å‘Š

```bash
#!/bin/bash
# scripts/reports/daily-ops-report.sh

REPORT_DATE=$(date +%Y-%m-%d)
REPORT_FILE="/var/log/daily-ops-report-$REPORT_DATE.md"

# ç”ŸæˆæŠ¥å‘Šå¤´éƒ¨
cat > $REPORT_FILE << EOF
# æ¯æ—¥è¿ç»´æŠ¥å‘Š - $REPORT_DATE

## ç³»ç»ŸçŠ¶æ€æ¦‚è§ˆ

EOF

# ç³»ç»Ÿèµ„æºçŠ¶æ€
echo "### ç³»ç»Ÿèµ„æº" >> $REPORT_FILE
echo "- **CPU ä½¿ç”¨ç‡**: $(top -bn1 | grep "Cpu(s)" | sed "s/.*, *\([0-9.]*\)%* id.*/\1/" | awk '{print 100 - $1}')%" >> $REPORT_FILE
echo "- **å†…å­˜ä½¿ç”¨ç‡**: $(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')%" >> $REPORT_FILE
echo "- **ç£ç›˜ä½¿ç”¨ç‡**: $(df / | tail -1 | awk '{print $5}')%" >> $REPORT_FILE

# æœåŠ¡çŠ¶æ€
echo "" >> $REPORT_FILE
echo "### æœåŠ¡çŠ¶æ€" >> $REPORT_FILE

services=("api" "frontend" "database" "redis")
for service in "${services[@]}"; do
    if docker-compose ps $service | grep -q "Up"; then
        status="âœ… æ­£å¸¸"
    else
        status="âŒ å¼‚å¸¸"
    fi
    echo "- **$service**: $status" >> $REPORT_FILE
done

# å¤‡ä»½çŠ¶æ€
echo "" >> $REPORT_FILE
echo "### å¤‡ä»½çŠ¶æ€" >> $REPORT_FILE

LATEST_BACKUP=$(ls -t /opt/fitness-gym/backups/*.sql 2>/dev/null | head -1)
if [ -n "$LATEST_BACKUP" ]; then
    BACKUP_TIME=$(stat -c %y "$LATEST_BACKUP" | cut -d'.' -f1)
    BACKUP_SIZE=$(du -h "$LATEST_BACKUP" | cut -f1)
    echo "- **æœ€æ–°å¤‡ä»½**: $BACKUP_TIME ($BACKUP_SIZE)" >> $REPORT_FILE
else
    echo "- **æœ€æ–°å¤‡ä»½**: âŒ æœªæ‰¾åˆ°å¤‡ä»½æ–‡ä»¶" >> $REPORT_FILE
fi

# å‘Šè­¦äº‹ä»¶
echo "" >> $REPORT_FILE
echo "### å‘Šè­¦äº‹ä»¶" >> $REPORT_FILE

ALERT_COUNT=$(grep "$(date +%Y-%m-%d)" /var/log/prometheus/alerts.log 2>/dev/null | wc -l)
echo "- **å‘Šè­¦æ•°é‡**: $ALERT_COUNT" >> $REPORT_FILE

if [ $ALERT_COUNT -gt 0 ]; then
    echo "" >> $REPORT_FILE
    echo "#### å‘Šè­¦è¯¦æƒ…" >> $REPORT_FILE
    grep "$(date +%Y-%m-%d)" /var/log/prometheus/alerts.log 2>/dev/null | head -10 >> $REPORT_FILE
fi

# æ€§èƒ½æŒ‡æ ‡
echo "" >> $REPORT_FILE
echo "### æ€§èƒ½æŒ‡æ ‡" >> $REPORT_FILE

# API å“åº”æ—¶é—´
API_RESPONSE=$(curl -s -w "%{time_total}" -o /dev/null http://localhost:3000/health)
echo "- **API å“åº”æ—¶é—´**: ${API_RESPONSE}s" >> $REPORT_FILE

# æ•°æ®åº“è¿æ¥æ•°
DB_CONNECTIONS=$(docker-compose exec -T database psql -U fitness_user -d fitness_gym -c "SELECT count(*) FROM pg_stat_activity;" -t | tr -d ' ')
echo "- **æ•°æ®åº“è¿æ¥æ•°**: $DB_CONNECTIONS" >> $REPORT_FILE

echo "" >> $REPORT_FILE
echo "*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: $(date)*" >> $REPORT_FILE

echo "ğŸ“„ æ¯æ—¥è¿ç»´æŠ¥å‘Šå·²ç”Ÿæˆ: $REPORT_FILE"

# å‘é€æŠ¥å‘Šé‚®ä»¶
if [ -n "$ADMIN_EMAIL" ]; then
    mail -s "å¥èº«æˆ¿ç³»ç»Ÿæ¯æ—¥è¿ç»´æŠ¥å‘Š - $REPORT_DATE" $ADMIN_EMAIL < $REPORT_FILE
fi
```

---

## æ€»ç»“

æœ¬è‡ªåŠ¨åŒ–è¿ç»´æŒ‡å—æ¶µç›–äº†ç°ä»£ DevOps å®è·µçš„æ ¸å¿ƒç»„ä»¶ï¼š

### ğŸ”„ CI/CD æµæ°´çº¿
- è‡ªåŠ¨åŒ–ä»£ç è´¨é‡æ£€æŸ¥å’Œæµ‹è¯•
- å¤šç¯å¢ƒè‡ªåŠ¨éƒ¨ç½²
- å›æ»šå’Œè“ç»¿éƒ¨ç½²æ”¯æŒ

### ğŸ“Š ç›‘æ§å’Œå‘Šè­¦
- Prometheus + Grafana ç›‘æ§æ ˆ
- æ™ºèƒ½å‘Šè­¦è§„åˆ™å’Œè‡ªåŠ¨å“åº”
- å…¨é¢çš„ç³»ç»Ÿå’Œåº”ç”¨æŒ‡æ ‡

### ğŸ’¾ å¤‡ä»½å’Œæ¢å¤
- è‡ªåŠ¨åŒ–æ•°æ®åº“å¤‡ä»½ç­–ç•¥
- å¤‡ä»½å®Œæ•´æ€§éªŒè¯
- ç¾éš¾æ¢å¤æ¼”ç»ƒæµç¨‹

### ğŸ—ï¸ åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–
- Docker å®¹å™¨ç¼–æ’
- è‡ªåŠ¨æ‰©ç¼©å®¹
- å¥åº·æ£€æŸ¥å’Œè‡ªæ„ˆ

### ğŸ”’ å®‰å…¨è‡ªåŠ¨åŒ–
- å®¹å™¨é•œåƒå®‰å…¨æ‰«æ
- è‡ªåŠ¨åŒ–å¯†é’¥è½®æ¢
- ä¾èµ–æ¼æ´æ£€æŸ¥

### ğŸ“š æ–‡æ¡£è‡ªåŠ¨åŒ–
- API æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ
- éƒ¨ç½²æ–‡æ¡£ç»´æŠ¤
- æ–‡æ¡£éªŒè¯å’Œå‘å¸ƒ

### ğŸ”§ è¿ç»´æµç¨‹è‡ªåŠ¨åŒ–
- å®šæœŸç»´æŠ¤ä»»åŠ¡
- å®¹é‡è§„åˆ’ç›‘æ§
- è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ

é€šè¿‡å®æ–½è¿™äº›è‡ªåŠ¨åŒ–æªæ–½ï¼Œç³»ç»Ÿèƒ½å¤Ÿå®ç°ï¼š
- **7Ã—24 å°æ—¶ç¨³å®šè¿è¡Œ**
- **å¿«é€Ÿæ•…éšœæ£€æµ‹å’Œæ¢å¤**
- **æŒç»­çš„æ€§èƒ½ä¼˜åŒ–**
- **è‡ªåŠ¨åŒ–çš„å®‰å…¨é˜²æŠ¤**
- **æ ‡å‡†åŒ–çš„è¿ç»´æµç¨‹**

---

*æœ€åæ›´æ–°: 2025-11-16*  
*ç‰ˆæœ¬: v1.0*  
*ç»´æŠ¤è€…: è¿ç»´å›¢é˜Ÿ*
