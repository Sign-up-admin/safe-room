#!/usr/bin/env tsx

/**
 * æµ‹è¯•æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥è„šæœ¬
 * ç”¨äºCIéªŒè¯ï¼Œç¡®ä¿æµ‹è¯•æ•°æ®ç¬¦åˆè§„èŒƒ
 */

import * as fs from 'fs'
import * as path from 'path'
import { glob } from 'glob'
import { MockDataValidator, generateValidationReport } from '../tests/shared/utils/mock-validator'
import { factoryRegistry } from '../tests/shared/registry/factory-registry'

// ========== é…ç½® ==========

const CONFIG = {
  // æµ‹è¯•æ–‡ä»¶ç›®å½•
  testDirs: [
    'springboot1ngh61a2/src/main/resources/front/front/tests',
    'springboot1ngh61a2/src/main/resources/admin/admin/tests'
  ],

  // éœ€è¦æ£€æŸ¥çš„æ–‡ä»¶æ‰©å±•å
  extensions: ['.ts', '.js'],

  // æ’é™¤çš„æ–‡ä»¶å’Œç›®å½•
  excludePatterns: [
    '**/node_modules/**',
    '**/dist/**',
    '**/build/**',
    '**/*.d.ts',
    '**/setup/**',
    '**/mocks/**',
    '**/fixtures/**'
  ],

  // è¾“å‡ºæ–‡ä»¶
  outputFile: 'test-data-validation-report.json',
  reportDir: 'docs/reports/test-validation'
}

// ========== ç±»å‹å®šä¹‰ ==========

interface ValidationIssue {
  file: string
  line: number
  column: number
  type: 'hardcoded_data' | 'invalid_mock' | 'missing_factory' | 'validation_error'
  severity: 'error' | 'warning' | 'info'
  message: string
  suggestion?: string
}

interface ValidationResult {
  file: string
  issues: ValidationIssue[]
  score: number // 0-100, è´¨é‡è¯„åˆ†
  stats: {
    totalLines: number
    hardcodedDataCount: number
    mockUsageCount: number
    factoryUsageCount: number
  }
}

interface ValidationReport {
  timestamp: string
  summary: {
    totalFiles: number
    filesWithIssues: number
    totalIssues: number
    averageScore: number
    issuesByType: Record<string, number>
    issuesBySeverity: Record<string, number>
  }
  results: ValidationResult[]
  recommendations: string[]
}

// ========== å·¥å…·å‡½æ•° ==========

/**
 * æŸ¥æ‰¾æ‰€æœ‰æµ‹è¯•æ–‡ä»¶
 */
async function findTestFiles(): Promise<string[]> {
  const patterns = CONFIG.testDirs.flatMap(dir =>
    CONFIG.extensions.map(ext => path.join(dir, `**/*${ext}`))
  )

  const allFiles: string[] = []
  for (const pattern of patterns) {
    const files = await glob(pattern, {
      ignore: CONFIG.excludePatterns,
      absolute: true
    })
    allFiles.push(...files)
  }

  return [...new Set(allFiles)] // å»é‡
}

/**
 * è¯»å–æ–‡ä»¶å†…å®¹
 */
function readFileContent(filePath: string): string {
  try {
    return fs.readFileSync(filePath, 'utf-8')
  } catch (error) {
    console.warn(`Failed to read file: ${filePath}`, error)
    return ''
  }
}

/**
 * è§£æä»£ç è¡Œå·
 */
function getLineNumber(content: string, index: number): number {
  const lines = content.substring(0, index).split('\n')
  return lines.length
}

/**
 * è§£æåˆ—å·
 */
function getColumnNumber(content: string, index: number): number {
  const lines = content.substring(0, index).split('\n')
  return lines[lines.length - 1].length + 1
}

// ========== éªŒè¯è§„åˆ™ ==========

/**
 * æ£€æŸ¥ç¡¬ç¼–ç æ•°æ®
 */
function checkHardcodedData(content: string): ValidationIssue[] {
  const issues: ValidationIssue[] = []
  const lines = content.split('\n')

  // åŒ¹é…ç¡¬ç¼–ç çš„å¯¹è±¡æ•°æ®
  const hardcodedPatterns = [
    // ç¡¬ç¼–ç çš„ç”¨æˆ·æ•°æ®
    /{[^}]*\bid\s*:\s*\d+[^}]*\bname\s*:\s*['"][^'"]*['"][^}]*}/g,
    // ç¡¬ç¼–ç çš„ID
    /\bid\s*:\s*\d+/g,
    // ç¡¬ç¼–ç çš„é‚®ç®±
    /\bemail\s*:\s*['"][^'"]*@[^'"]*['"]/g,
    // ç¡¬ç¼–ç çš„å“åº”æ•°æ®
    /mockResolvedValueOnce\s*\(\s*\{\s*data\s*:\s*\{[^}]*\}\s*\}\s*\)/g
  ]

  lines.forEach((line, lineIndex) => {
    hardcodedPatterns.forEach(pattern => {
      const matches = line.match(pattern)
      if (matches) {
        matches.forEach(match => {
          // æ£€æŸ¥æ˜¯å¦å·²ç»ä½¿ç”¨äº†å·¥å‚å‡½æ•°
          if (!line.includes('createMock') && !line.includes('createValidated')) {
            issues.push({
              file: '', // å°†åœ¨è°ƒç”¨æ—¶è®¾ç½®
              line: lineIndex + 1,
              column: line.indexOf(match) + 1,
              type: 'hardcoded_data',
              severity: 'warning',
              message: `Found hardcoded test data: ${match.substring(0, 50)}...`,
              suggestion: 'Consider using factory functions like createMockUser() or createMockCourse()'
            })
          }
        })
      }
    })
  })

  return issues
}

/**
 * æ£€æŸ¥Mockä½¿ç”¨æƒ…å†µ
 */
function checkMockUsage(content: string): ValidationIssue[] {
  const issues: ValidationIssue[] = []
  const lines = content.split('\n')

  lines.forEach((line, lineIndex) => {
    // æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ—§çš„Mockæ–¹å¼
    if (line.includes('mockResolvedValueOnce') && line.includes('{ data:')) {
      // æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ–°çš„å“åº”ç”Ÿæˆå™¨
      if (!line.includes('createApiResponse') && !line.includes('createListResponse')) {
        issues.push({
          file: '',
          line: lineIndex + 1,
          column: 1,
          type: 'invalid_mock',
          severity: 'warning',
          message: 'Mock response not using standardized format',
          suggestion: 'Use createApiResponse() or createListResponse() for consistent API responses'
        })
      }
    }
  })

  return issues
}

/**
 * æ£€æŸ¥å·¥å‚å‡½æ•°ä½¿ç”¨æƒ…å†µ
 */
function checkFactoryUsage(content: string): ValidationIssue[] {
  const issues: ValidationIssue[] = []
  const lines = content.split('\n')

  // æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•æ•°æ®ä½†æ²¡æœ‰ä½¿ç”¨å·¥å‚
  const hasTestData = lines.some(line =>
    line.includes('describe(') || line.includes('it(') || line.includes('test(')
  )

  const hasFactoryUsage = lines.some(line =>
    line.includes('createMock') || line.includes('createValidated')
  )

  if (hasTestData && !hasFactoryUsage) {
    issues.push({
      file: '',
      line: 1,
      column: 1,
      type: 'missing_factory',
      severity: 'info',
      message: 'Test file contains test data but no factory functions detected',
      suggestion: 'Consider importing and using factory functions for consistent test data'
    })
  }

  return issues
}

// ========== ä¸»è¦éªŒè¯å‡½æ•° ==========

/**
 * éªŒè¯å•ä¸ªæ–‡ä»¶
 */
function validateFile(filePath: string): ValidationResult {
  const content = readFileContent(filePath)
  const lines = content.split('\n')

  const issues: ValidationIssue[] = []

  // è¿è¡Œå„ç§æ£€æŸ¥
  issues.push(...checkHardcodedData(content).map(issue => ({ ...issue, file: filePath })))
  issues.push(...checkMockUsage(content).map(issue => ({ ...issue, file: filePath })))
  issues.push(...checkFactoryUsage(content).map(issue => ({ ...issue, file: filePath })))

  // è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
  const stats = {
    totalLines: lines.length,
    hardcodedDataCount: issues.filter(i => i.type === 'hardcoded_data').length,
    mockUsageCount: (content.match(/mockResolvedValueOnce/g) || []).length,
    factoryUsageCount: (content.match(/createMock|createValidated/g) || []).length
  }

  // è®¡ç®—è´¨é‡è¯„åˆ† (0-100)
  const score = calculateQualityScore(stats, issues)

  return {
    file: filePath,
    issues,
    score,
    stats
  }
}

/**
 * è®¡ç®—è´¨é‡è¯„åˆ†
 */
function calculateQualityScore(stats: ValidationResult['stats'], issues: ValidationIssue[]): number {
  let score = 100

  // ç¡¬ç¼–ç æ•°æ®ä¸¥é‡æ‰£åˆ†
  score -= issues.filter(i => i.type === 'hardcoded_data').length * 5

  // æ— æ•ˆMockæ‰£åˆ†
  score -= issues.filter(i => i.type === 'invalid_mock').length * 3

  // å·¥å‚å‡½æ•°ä½¿ç”¨åŠ åˆ†
  if (stats.factoryUsageCount > 0) {
    score += Math.min(stats.factoryUsageCount * 2, 20)
  }

  // åŸºäºä»£ç è¡Œæ•°çš„æƒ©ç½šï¼ˆè¿‡é•¿çš„æ–‡ä»¶å¯èƒ½éœ€è¦é‡æ„ï¼‰
  if (stats.totalLines > 500) {
    score -= 10
  }

  return Math.max(0, Math.min(100, score))
}

// ========== æŠ¥å‘Šç”Ÿæˆ ==========

/**
 * ç”ŸæˆéªŒè¯æŠ¥å‘Š
 */
function generateReport(results: ValidationResult[]): ValidationReport {
  const totalFiles = results.length
  const filesWithIssues = results.filter(r => r.issues.length > 0).length
  const totalIssues = results.reduce((sum, r) => sum + r.issues.length, 0)
  const averageScore = results.reduce((sum, r) => sum + r.score, 0) / totalFiles

  const issuesByType: Record<string, number> = {}
  const issuesBySeverity: Record<string, number> = {}

  results.forEach(result => {
    result.issues.forEach(issue => {
      issuesByType[issue.type] = (issuesByType[issue.type] || 0) + 1
      issuesBySeverity[issue.severity] = (issuesBySeverity[issue.severity] || 0) + 1
    })
  })

  const recommendations = generateRecommendations(results, {
    totalFiles,
    filesWithIssues,
    totalIssues,
    averageScore,
    issuesByType,
    issuesBySeverity
  })

  return {
    timestamp: new Date().toISOString(),
    summary: {
      totalFiles,
      filesWithIssues,
      totalIssues,
      averageScore,
      issuesByType,
      issuesBySeverity
    },
    results,
    recommendations
  }
}

/**
 * ç”Ÿæˆæ”¹è¿›å»ºè®®
 */
function generateRecommendations(
  results: ValidationResult[],
  summary: ValidationReport['summary']
): string[] {
  const recommendations: string[] = []

  if (summary.averageScore < 70) {
    recommendations.push('Overall test data quality needs improvement')
  }

  if (summary.issuesByType.hardcoded_data > summary.totalFiles * 0.5) {
    recommendations.push('High number of hardcoded data detected. Consider implementing more factory functions')
  }

  if (summary.issuesByType.invalid_mock > 0) {
    recommendations.push('Some tests are not using standardized Mock response format. Update to use createApiResponse()')
  }

  if (summary.issuesByType.missing_factory > summary.totalFiles * 0.3) {
    recommendations.push('Many test files lack factory function usage. Consider adding factory imports and usage')
  }

  const lowScoreFiles = results.filter(r => r.score < 60).map(r => path.basename(r.file))
  if (lowScoreFiles.length > 0) {
    recommendations.push(`Files with low quality scores: ${lowScoreFiles.join(', ')}`)
  }

  return recommendations
}

// ========== ä¸»å‡½æ•° ==========

/**
 * ä¸»éªŒè¯å‡½æ•°
 */
async function main() {
  console.log('ğŸ” Starting test data validation...')

  // åˆå§‹åŒ–å·¥å‚æ³¨å†Œè¡¨
  console.log('ğŸ“‹ Initializing factory registry...')
  factoryRegistry.clear()
  // è¿™é‡Œå¯ä»¥åˆå§‹åŒ–é¢„å®šä¹‰çš„å·¥å‚

  // æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
  console.log('ğŸ” Finding test files...')
  const testFiles = await findTestFiles()
  console.log(`ğŸ“ Found ${testFiles.length} test files`)

  // éªŒè¯æ¯ä¸ªæ–‡ä»¶
  console.log('âš¡ Validating files...')
  const results: ValidationResult[] = []

  for (const file of testFiles) {
    console.log(`  Checking: ${path.relative(process.cwd(), file)}`)
    const result = validateFile(file)
    results.push(result)

    if (result.issues.length > 0) {
      console.log(`    âš ï¸  Found ${result.issues.length} issues (Score: ${result.score})`)
    } else {
      console.log(`    âœ… No issues found (Score: ${result.score})`)
    }
  }

  // ç”ŸæˆæŠ¥å‘Š
  console.log('ğŸ“Š Generating report...')
  const report = generateReport(results)

  // ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
  const outputDir = path.join(process.cwd(), CONFIG.reportDir)
  if (!fs.existsSync(outputDir)) {
    fs.mkdirSync(outputDir, { recursive: true })
  }

  // å†™å…¥æŠ¥å‘Šæ–‡ä»¶
  const outputPath = path.join(outputDir, CONFIG.outputFile)
  fs.writeFileSync(outputPath, JSON.stringify(report, null, 2), 'utf-8')

  // ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
  const textReportPath = path.join(outputDir, 'validation-report.md')
  const textReport = generateTextReport(report)
  fs.writeFileSync(textReportPath, textReport, 'utf-8')

  // è¾“å‡ºæ€»ç»“
  console.log('\nğŸ“ˆ Validation Summary:')
  console.log(`   Total files: ${report.summary.totalFiles}`)
  console.log(`   Files with issues: ${report.summary.filesWithIssues}`)
  console.log(`   Total issues: ${report.summary.totalIssues}`)
  console.log(`   Average score: ${report.summary.averageScore.toFixed(1)}`)
  console.log(`   Reports saved to: ${outputDir}`)

  if (report.recommendations.length > 0) {
    console.log('\nğŸ’¡ Recommendations:')
    report.recommendations.forEach(rec => console.log(`   â€¢ ${rec}`))
  }

  // è¿”å›é€€å‡ºç 
  const hasErrors = report.results.some(r => r.issues.some(i => i.severity === 'error'))
  process.exit(hasErrors ? 1 : 0)
}

/**
 * ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
 */
function generateTextReport(report: ValidationReport): string {
  let text = '# æµ‹è¯•æ•°æ®éªŒè¯æŠ¥å‘Š\n\n'

  text += `ç”Ÿæˆæ—¶é—´: ${new Date(report.timestamp).toLocaleString('zh-CN')}\n\n`

  text += '## ğŸ“Š æ€»è§ˆ\n\n'
  text += `| æŒ‡æ ‡ | æ•°å€¼ |\n`
  text += `|------|------|\n`
  text += `| æ€»æ–‡ä»¶æ•° | ${report.summary.totalFiles} |\n`
  text += `| æœ‰é—®é¢˜æ–‡ä»¶æ•° | ${report.summary.filesWithIssues} |\n`
  text += `| æ€»é—®é¢˜æ•° | ${report.summary.totalIssues} |\n`
  text += `| å¹³å‡è´¨é‡è¯„åˆ† | ${report.summary.averageScore.toFixed(1)} |\n`
  text += '\n'

  if (Object.keys(report.summary.issuesByType).length > 0) {
    text += '## ğŸ” é—®é¢˜ç±»å‹åˆ†å¸ƒ\n\n'
    text += `| é—®é¢˜ç±»å‹ | æ•°é‡ |\n`
    text += `|----------|------|\n`
    Object.entries(report.summary.issuesByType).forEach(([type, count]) => {
      text += `| ${type} | ${count} |\n`
    })
    text += '\n'
  }

  if (report.recommendations.length > 0) {
    text += '## ğŸ’¡ æ”¹è¿›å»ºè®®\n\n'
    report.recommendations.forEach(rec => {
      text += `- ${rec}\n`
    })
    text += '\n'
  }

  if (report.results.some(r => r.issues.length > 0)) {
    text += '## ğŸ“‹ è¯¦ç»†é—®é¢˜\n\n'

    report.results
      .filter(r => r.issues.length > 0)
      .forEach(result => {
        text += `### ${path.relative(process.cwd(), result.file)}\n\n`
        text += `è´¨é‡è¯„åˆ†: ${result.score.toFixed(1)}\n\n`

        result.issues.forEach(issue => {
          const severityIcon = {
            error: 'âŒ',
            warning: 'âš ï¸',
            info: 'â„¹ï¸'
          }[issue.severity] || 'â“'

          text += `${severityIcon} **${issue.type}** (è¡Œ ${issue.line}): ${issue.message}\n\n`
          if (issue.suggestion) {
            text += `   ğŸ’¡ ${issue.suggestion}\n\n`
          }
        })

        text += '---\n\n'
      })
  }

  return text
}

// ========== æ‰§è¡Œè„šæœ¬ ==========

if (require.main === module) {
  main().catch(error => {
    console.error('âŒ Validation failed:', error)
    process.exit(1)
  })
}

export { main as validateTestData }
